New (Create) – In this step, the process is about to be created but not yet created, it is the program which is present in secondary memory that will be picked up by OS to create the process.

Ready – New -> Ready to run. After the creation of a process, the process enters the ready state i.e. the process is loaded into the main memory. The process here is ready to run and is waiting to get the CPU time for its execution. Processes that are ready for execution by the CPU are maintained in a queue for ready processes.

Run – The process is chosen by CPU for execution and the instructions within the process are executed by any one of the available CPU cores.

Blocked or wait – Whenever the process requests access to I/O or needs input from the user or needs access to a critical region(the lock for which is already acquired) it enters the blocked or wait state. The process continues to wait in the main memory and does not require CPU. Once the I/O operation is completed the process goes to the ready state.

Terminated or completed – Process is killed as well as PCB is deleted.

Suspend ready – Process that was initially in the ready state but were swapped out of main memory(refer Virtual Memory topic) and placed onto external storage by scheduler are said to be in suspend ready state. The process will transition back to ready state whenever the process is again brought onto the main memory.

Suspend wait or suspend blocked – Similar to suspend ready but uses the process which was performing I/O operation and lack of main memory caused them to move to secondary memory.
When work is finished it may go to suspend ready.


-------------------------------------------------------------------------------------------------------------------------------------------


PROCESS/JOBS/TASK/THREAD/THREAD-POOL


Process is a well-defined operating systems concept, as is thread: a process is an instance of a program that is being executed, and is the basic unit of resources: a process consists of or “owns” its image, execution context, memory, files, etc.

A process consists of one or more threads, which are the unit of scheduling, and consist of some subset of a process (possibly shared with other threads): execution context and perhaps more. Traditionally a thread is the unit of execution on a processor (a thread is “what is executing”)

The term “job” traditionally means a “piece of work”.In computing, “job” originates in non-interactive processing on mainframes.Early computers primarily did batch processing and a standard type of one-off job was compiling a program from source, which could then process batches of data. 

In Unix shells, a “job” is the shell’s representation for a process group – a set of processes that can all be sent a signal – concretely a pipeline and its descendent processes; note that running a script starts a job, exactly as in mainframes. The job is not done until the processes complete, and a job can be stopped, resumed, or terminated, which corresponds to suspending, resuming, or terminating the processes. Thus while formally a job is distinct from the process group, this is a subtle distinction and thus people often use “job” to mean “set of processes”.

In computer programming, a thread pool is a software design pattern for achieving concurrency of execution in a computer program. Often also called a replicated workers or worker-crew model,[1] a thread pool maintains multiple threads waiting for tasks to be allocated for concurrent execution by the supervising program. By maintaining a pool of threads, the model increases performance and avoids latency in execution due to frequent creation and destruction of threads for short-lived tasks.[2] The number of available threads is tuned to the computing resources available to the program, such as a parallel task queue after completion of execution.

UNIX has separate concepts "process", "process group", and "session".The shell creates a process group within the current session for each "job" it launches, and places each process it starts into the appropriate process group. For example, ls | head is a pipeline of two processes, which the shell considers a single job, and will belong to a single, new process group.

A process is a (collection of) thread of execution and other context, such as address space and file descriptor table. A process may start other processes; these new processes will belong to the same process group as the parent unless other action is taken. Each process may also have a "controlling terminal", which starts off the same as its parent.


-------------------------------------------------------------------------------------------------------------------------------------------

Dining Philosophers Problem:

Some of the ways to avoid deadlock are as follows:

* There should be at most four philosophers on the table.
* An even philosopher should pick the right chopstick and then the left chopstick while an odd philosopher should pick the left chopstick and 	then the right chopstick.
* A philosopher should only be allowed to pick their chopstick if both are available at the same time.

-------------------------------------------------------------------------------------------------------------------------------------------

Mutex v/ Semaphore

There are many issues unique to multithreaded programming, the most prominent being atomicity and ordering. Whats important to realize is that these are completely unrelated to each other.

# Atomicity:
An illusion that a section of code either executes completely, or doesn't execute at all. This illusion can be provided by allowing only one thread to execute that code at a time. Atomicity is a key requirement for generating consistent results with respect to a memory location. It is particularly useful for the parts of your program that modify shared state, say globals.

# Ordering:
Different threads might be running on different cores. However, since there is no such thing as a global clock, at times its imperative to achieve ordering of instructions across various cores, for correctness. For example, it might be a correctness requirement to execute instruction X of thread T1 running on core C1 before instruction Y of thread T2 running on core C2.

A mutex is used to meet the atomicity requirement. However, it does not impose any ordering. In other words, given two threads, use of a mutex can't specify, which thread will acquire the mutex first, and hence execute the critical section before the other. The only assurance is that if one thread is executing the critical section, the other will be kept out of it.

On the other hand, a semaphore can be used to impose ordering constraints in your execution. Considering the aforementioned example, you can block thread T2 just before it executes instruction Y, conditioned on whether T1 is done executing instruction X. This can be done by making T2 wait on the same condition variable that T1 signals, precisely the programming abstraction that the semaphore provides through its wait() and signal() operations.

Thus, a mutex can only be used to maintain atomicity whereas a semaphore can be used for both ordering and atomicity.

-------------------------------------------------------------------------------------------------------------------------------------------

Readers - Writers problem:


# The Problem Statement

There is a shared resource which should be accessed by multiple processes. There are two types of processes in this context. They are reader and writer. Any number of readers can read from the shared resource simultaneously, but only one writer can write to the shared resource. When a writer is writing data to the resource, no other process can access the resource. A writer cannot write to the resource if there are non zero number of readers accessing the resource at that time.

The Solution
From the above problem statement, it is evident that readers have higher priority than writer. If a writer wants to write to the resource, it must wait until there are no readers currently accessing that resource.

Here, we use one mutex m and a semaphore w. An integer variable read_count is used to maintain the number of readers currently accessing the resource. The variable read_count is initialized to 0. A value of 1 is given initially to m and w.

Instead of having the process to acquire lock on the shared resource, we use the mutex m to make the process to acquire and release lock whenever it is updating the read_count variable.


The code for the writer process looks like this:

while(TRUE) 
{
    wait(w);
    
   /* perform the write operation */
   
   signal(w);
}


And, the code for the reader process looks like this:

while(TRUE) 
{
    //acquire lock
    wait(m);
    read_count++;
    if(read_count == 1)
        wait(w);                               // puts a halt on all other writers 
    
    //release lock  
    signal(m);  
    
    /* perform the reading operation */
    
    // acquire lock
    wait(m);   
    read_count--;
    if(read_count == 0)
        signal(w);			      // all writers can be made available again
        
    // release lock
    signal(m);  
} 



-------------------------------------------------------------------------------------------------------------------------------------------

Bounded Buffer Problem


Bounded buffer problem, which is also called producer consumer problem, is one of the classic problems of synchronization. 

There is a buffer of n slots and each slot is capable of storing one unit of data. There are two processes running, namely, producer and consumer, which are operating on the buffer.

A producer tries to insert data into an empty slot of the buffer. A consumer tries to remove data from a filled slot in the buffer. As you might have guessed by now, those two processes won't produce the expected output if they are being executed concurrently.

One solution of this problem is to use semaphores. 

The semaphores which will be used here are:

i>   m, a binary semaphore which is used to acquire and release the lock.
ii>  empty, a counting semaphore whose initial value is the number of slots in the buffer, since, initially all slots are empty.
iii> full, a counting semaphore whose initial value is 0.


The pseudocode of the producer function looks like this:

do 
{
    // wait until empty > 0 and then decrement 'empty'
    wait(empty);   
    // acquire lock
    wait(mutex);  
    
    /* perform the insert operation in a slot */
    
    // release lock
    signal(mutex);  
    // increment 'full'
    signal(full);   
} 
while(TRUE)


The pseudocode for the consumer function looks like this:

do 
{
    // wait until full > 0 and then decrement 'full'
    wait(full);
    // acquire the lock
    wait(mutex);  
    
    /* perform the remove operation in a slot */ 
    
    // release the lock
    signal(mutex); 
    // increment 'empty'
    signal(empty); 
} 
while(TRUE);

-------------------------------------------------------------------------------------------------------------------------------------------

Deadlock avoidance v/s Deadlock prevention:

Deadlock prevention means to block at least one of the following four conditions required for deadlock to occur.

Mutual Exclusion
Hold and Wait
No Preemption
Circular Wait

In Deadlock avoidance we have to anticipate deadlock before it really occurs and ensure that the system does not go in unsafe state.It is possible to avoid deadlock if resources are allocated carefully. For deadlock avoidance we use Banker’s and Safety algorithm for resource allocation purpose. In deadlock avoidance the maximum number of resources of each type that will be needed are stated at the beginning of the process.

-------------------------------------------------------------------------------------------------------------------------------------------

Address Binding:

Computer memory uses both logical addresses and physical addresses. Address binding allocates a physical memory location to a logical pointer by associating a physical address to a logical address, which is also known as a virtual address. Address binding is part of computer memory management and it is performed by the operating system on behalf of the applications that need access to memory.

There are 3 types of Address Binding:

i>   Compile Time Address Binding
ii>  Load Time Address Binding
iii> Execution Time Address Binding


Compile Time Address Binding:

If the compiler is responsible of performing address binding then it is called as compile time address binding. This type of address binding will be done before loading the program into memory. The compiler required to interact with the operating system memory manager to perform compile time address binding.


Load Time Address Binding:

This type of address binding will be done after loading the program into memory. Load time address binding will be done by operating memory manager.

Execution Time:

Execution time address binding usually applies only to variables in programs and is the most common form of binding for scripts, which don't get compiled. In this scenario, the program requests memory space for a variable in a program the first time that variable is encountered during the processing of instructions in the script. The memory will allocate space to that variable until the program sequence ends, or unless a specific instruction within the script releases the memory address bound to a variable.

-------------------------------------------------------------------------------------------------------------------------------------------

Dynamic Loading v/s Dynamic Linking:

Dynamic loading refers to mapping (or less often copying) an executable or library into a process's memory after is has started. Dynamic linking refers to resolving symbols - associating their names with addresses or offsets - after compile time. 


Overlays:

Overlay is a technique to run a program that is bigger than the size of the physical memory by keeping only those instructions and data that are needed at any given time.Divide the program into modules in such a way that not all modules need to be in the memory at the same time.


-------------------------------------------------------------------------------------------------------------------------------------------

Paging:

In computer operating systems, paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory. In this scheme, the operating system retrieves data from secondary storage in same-size blocks called pages. Paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. This scheme permits the physical address space of a process to be non – contiguous.

-------------------------------------------------------------------------------------------------------------------------------------------
worm v/s virus v/s trojan horse

https://www.websecurity.digicert.com/security-topics/difference-between-virus-worm-and-trojan-horse#:~:text=A%20Trojan%20horse%20is%20not%20a%20virus.&text=Unlike%20viruses%2C%20Trojan%20horses%20do,personal%20information%20to%20be%20theft.


https://www.geeksforgeeks.org/difference-between-virus-worm-and-trojan-horse/


-------------------------------------------------------------------------------------------------------------------------------------------

What is a Kernel? 

A Kernel is a computer program that is the heart and core of an Operating System. Since the Operating System has control over the system so, the Kernel also has control over everything in the system. It is the most important part of an Operating System. Whenever a system starts, the Kernel is the first program that is loaded after the bootloader because the Kernel has to handle the rest of the thing of the system for the Operating System. The Kernel remains in the memory until the Operating System is shut-down.

The Kernel is responsible for low-level tasks such as disk management, memory management, task management, etc. It provides an interface between the user and the hardware components of the system. When a process makes a request to the Kernel, then it is called System Call.

A Kernel is provided with a protected Kernel Space which is a separate area of memory and this area is not accessible by other application programs. So, the code of the Kernel is loaded into this protected Kernel Space. Apart from this, the memory used by other applications is called the User Space. As these are two different spaces in the memory, so communication between them is a bit slower.


System Library − System libraries are special functions or programs using which application programs or system utilities accesses Kernel's features. These libraries implement most of the functionalities of the operating system and do not requires kernel module's code access rights.


Kernel Mode vs User Mode: 

Kernel component code executes in a special privileged mode called kernel mode with full access to all resources of the computer. This code represents a single process, executes in single address space and do not require any context switch and hence is very efficient and fast. Kernel runs each processes and provides system services to processes, provides protected access to hardware to processes.

Support code which is not required to run in kernel mode is in System Library. User programs and other system programs works in User Mode which has no access to system hardware and kernel code. User programs/ utilities use System libraries to access Kernel functions to get system's low level tasks.


-------------------------------------------------------------------------------------------------------------------------------------------

