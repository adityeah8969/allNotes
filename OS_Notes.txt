New (Create) – In this step, the process is about to be created but not yet created, it is the program which is present in secondary memory that will be picked up by OS to create the process.

Ready – New -> Ready to run. After the creation of a process, the process enters the ready state i.e. the process is loaded into the main memory. The process here is ready to run and is waiting to get the CPU time for its execution. Processes that are ready for execution by the CPU are maintained in a queue for ready processes.

Run – The process is chosen by CPU for execution and the instructions within the process are executed by any one of the available CPU cores.

Blocked or wait – Whenever the process requests access to I/O or needs input from the user or needs access to a critical region(the lock for which is already acquired) it enters the blocked or wait state. The process continues to wait in the main memory and does not require CPU. Once the I/O operation is completed the process goes to the ready state.

Terminated or completed – Process is killed as well as PCB is deleted.

Suspend ready – Process that was initially in the ready state but were swapped out of main memory(refer Virtual Memory topic) and placed onto external storage by scheduler are said to be in suspend ready state. The process will transition back to ready state whenever the process is again brought onto the main memory.

Suspend wait or suspend blocked – Similar to suspend ready but uses the process which was performing I/O operation and lack of main memory caused them to move to secondary memory.
When work is finished it may go to suspend ready.


-------------------------------------------------------------------------------------------------------------------------------------------


PROCESS/JOBS/TASK/THREAD/THREAD-POOL


Process is a well-defined operating systems concept, as is thread: a process is an instance of a program that is being executed, and is the basic unit of resources: a process consists of or “owns” its image, execution context, memory, files, etc.

A process consists of one or more threads, which are the unit of scheduling, and consist of some subset of a process (possibly shared with other threads): execution context and perhaps more. Traditionally a thread is the unit of execution on a processor (a thread is “what is executing”)

The term “job” traditionally means a “piece of work”.In computing, “job” originates in non-interactive processing on mainframes.Early computers primarily did batch processing and a standard type of one-off job was compiling a program from source, which could then process batches of data. 

In Unix shells, a “job” is the shell’s representation for a process group – a set of processes that can all be sent a signal – concretely a pipeline and its descendent processes; note that running a script starts a job, exactly as in mainframes. The job is not done until the processes complete, and a job can be stopped, resumed, or terminated, which corresponds to suspending, resuming, or terminating the processes. Thus while formally a job is distinct from the process group, this is a subtle distinction and thus people often use “job” to mean “set of processes”.

In computer programming, a thread pool is a software design pattern for achieving concurrency of execution in a computer program. Often also called a replicated workers or worker-crew model,[1] a thread pool maintains multiple threads waiting for tasks to be allocated for concurrent execution by the supervising program. By maintaining a pool of threads, the model increases performance and avoids latency in execution due to frequent creation and destruction of threads for short-lived tasks.[2] The number of available threads is tuned to the computing resources available to the program, such as a parallel task queue after completion of execution.

UNIX has separate concepts "process", "process group", and "session".The shell creates a process group within the current session for each "job" it launches, and places each process it starts into the appropriate process group. For example, ls | head is a pipeline of two processes, which the shell considers a single job, and will belong to a single, new process group.

A process is a (collection of) thread of execution and other context, such as address space and file descriptor table. A process may start other processes; these new processes will belong to the same process group as the parent unless other action is taken. Each process may also have a "controlling terminal", which starts off the same as its parent.


-------------------------------------------------------------------------------------------------------------------------------------------

Dining Philosophers Problem:

do {
   wait( chopstick[i] );
   wait( chopstick[ (i+1) % 5] );
   . .
   . EATING THE RICE
   .
   signal( chopstick[i] );
   signal( chopstick[ (i+1) % 5] );
   .
   . THINKING
   .
} while(1);

But this solution can lead to a deadlock. This may happen if all the philosophers pick their left chopstick simultaneously. Then none of them can eat and deadlock occurs.

Some of the ways to avoid deadlock are as follows:

* There should be at most four philosophers on the table.
* An even philosopher should pick the right chopstick and then the left chopstick while an odd philosopher should pick the left chopstick and 	then the right chopstick.
* A philosopher should only be allowed to pick their chopstick if both are available at the same time.

-------------------------------------------------------------------------------------------------------------------------------------------

Mutex v/ Semaphore

Consider the standard producer-consumer problem. Assume, we have a buffer of 4096 byte length. A producer thread collects the data and writes it to the buffer. A consumer thread processes the collected data from the buffer. Objective is, both the threads should not run at the same time.

Using Mutex:

A mutex provides mutual exclusion, either producer or consumer can have the key (mutex) and proceed with their work. As long as the buffer is filled by producer, the consumer needs to wait, and vice versa.

At any point of time, only one thread can work with the entire buffer. The concept can be generalized using semaphore.

Using Semaphore:

A semaphore is a generalized mutex. In lieu of single buffer, we can split the 4 KB buffer into four 1 KB buffers (identical resources). A semaphore can be associated with these four buffers. The consumer and producer can work on different buffers at the same time.

Misconception:

There is an ambiguity between binary semaphore and mutex. We might have come across that a mutex is binary semaphore. But they are not! The purpose of mutex and semaphore are different. May be, due to similarity in their implementation a mutex would be referred as binary semaphore.


Strictly speaking, a mutex is locking mechanism used to synchronize access to a resource. Only one task (can be a thread or process based on OS abstraction) can acquire the mutex. It means there is ownership associated with mutex, and only the owner can release the lock (mutex).

Semaphore is signaling mechanism (“I am done, you can carry on” kind of signal). For example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend calls you, an interrupt is triggered upon which an interrupt service routine (ISR) signals the call processing task to wakeup.

-------------------------------------------------------------------------------------------------------------------------------------------

Readers - Writers problem:


# The Problem Statement

There is a shared resource which should be accessed by multiple processes. There are two types of processes in this context. They are reader and writer. Any number of readers can read from the shared resource simultaneously, but only one writer can write to the shared resource. When a writer is writing data to the resource, no other process can access the resource. A writer cannot write to the resource if there are non zero number of readers accessing the resource at that time.

The Solution
From the above problem statement, it is evident that readers have higher priority than writer. If a writer wants to write to the resource, it must wait until there are no readers currently accessing that resource.

Here, we use one mutex m and a semaphore w. An integer variable read_count is used to maintain the number of readers currently accessing the resource. The variable read_count is initialized to 0. A value of 1 is given initially to m and w.

read_count = 0
m = 1
w = 1

Instead of having the process to acquire lock on the shared resource, we use the mutex m to make the process to acquire and release lock whenever it is updating the read_count variable.

Note: The lock is getting acquired (on m) only when we need to update the read_count variable, not while reading.

The code for the writer process looks like this:

while(TRUE) 
{
    wait(w);
    
   /* perform the write operation */
   
   signal(w);
}


And, the code for the reader process looks like this:

while(TRUE) 
{
    //acquire lock
    wait(m);
    read_count++;
    if(read_count == 1)
        wait(w);                               // puts a halt on the writer
    
    //release lock  
    signal(m);  
    
    /* perform the reading operation */
    
    // acquire lock
    wait(m);   
    read_count--;
    if(read_count == 0)
        signal(w);			      // the writer can be made available again
        
    // release lock
    signal(m);  
} 


-------------------------------------------------------------------------------------------------------------------------------------------

Bounded Buffer Problem


Bounded buffer problem, which is also called producer consumer problem, is one of the classic problems of synchronization. 

There is a buffer of n slots and each slot is capable of storing one unit of data. There are two processes running, namely, producer and consumer, which are operating on the buffer.

A producer tries to insert data into an empty slot of the buffer. A consumer tries to remove data from a filled slot in the buffer. As you might have guessed by now, those two processes won't produce the expected output if they are being executed concurrently.

One solution of this problem is to use semaphores. 

The semaphores which will be used here are:

i>   m, a mutex which is used to acquire and release the lock.
ii>  empty, a counting semaphore whose initial value is the number of slots in the buffer, since, initially all slots are empty.
iii> full, a counting semaphore whose initial value is 0.


The pseudocode of the producer function looks like this:

do 
{
    // wait until empty > 0 and then decrement 'empty'
    wait(empty);   
    // acquire lock
    wait(mutex);  
    
    /* perform the insert operation in a slot */
    
    // release lock
    signal(mutex);  
    // increment 'full'
    signal(full);   
} 
while(TRUE)


The pseudocode for the consumer function looks like this:

do 
{
    // wait until full > 0 and then decrement 'full'
    wait(full);
    // acquire the lock
    wait(mutex);  
    
    /* perform the remove operation in a slot */ 
    
    // release the lock
    signal(mutex); 
    // increment 'empty'
    signal(empty); 
} 
while(TRUE);

-------------------------------------------------------------------------------------------------------------------------------------------

Deadlock avoidance v/s Deadlock prevention:

Deadlock prevention means to block at least one of the following four conditions required for deadlock to occur.

Mutual Exclusion (Each process executing the shared data (variables) excludes all others from doing so simultaneously)
Hold and Wait
Non Preemption (any new process has to wait until the running process finishes its CPU cycle)
Circular Wait

In Deadlock avoidance we have to anticipate deadlock before it really occurs and ensure that the system does not go in unsafe state.It is possible to avoid deadlock if resources are allocated carefully. For deadlock avoidance we use Banker’s and Safety algorithm for resource allocation purpose. In deadlock avoidance the maximum number of resources of each type that will be needed are stated at the beginning of the process.

-------------------------------------------------------------------------------------------------------------------------------------------

Address Binding:

Computer memory uses both logical addresses and physical addresses. Address binding allocates a physical memory location to a logical pointer by associating a physical address to a logical address, which is also known as a virtual address. Address binding is part of computer memory management and it is performed by the operating system on behalf of the applications that need access to memory.

There are 3 types of Address Binding:

i>   Compile Time Address Binding
ii>  Load Time Address Binding
iii> Execution Time Address Binding


Compile Time Address Binding:

If the compiler is responsible of performing address binding then it is called as compile time address binding. This type of address binding will be done before loading the program into memory. The compiler required to interact with the operating system memory manager to perform compile time address binding.


Load Time Address Binding:

This type of address binding will be done after loading the program into memory. Load time address binding will be done by operating memory manager.

Execution Time:

Execution time address binding usually applies only to variables in programs and is the most common form of binding for scripts, which don't get compiled. In this scenario, the program requests memory space for a variable in a program the first time that variable is encountered during the processing of instructions in the script. The memory will allocate space to that variable until the program sequence ends, or unless a specific instruction within the script releases the memory address bound to a variable.

-------------------------------------------------------------------------------------------------------------------------------------------

Dynamic Loading v/s Dynamic Linking:

Dynamic loading refers to mapping (or less often copying) an executable or library into a process's memory after is has started. Dynamic linking refers to resolving symbols - associating their names with addresses or offsets - after compile time. 


Overlays:

Overlay is a technique to run a program that is bigger than the size of the physical memory by keeping only those instructions and data that are needed at any given time.Divide the program into modules in such a way that not all modules need to be in the memory at the same time.


-------------------------------------------------------------------------------------------------------------------------------------------

Paging:

In computer operating systems, paging is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory. In this scheme, the operating system retrieves data from secondary storage in same-size blocks called pages. Paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. This scheme permits the physical address space of a process to be non – contiguous.

-------------------------------------------------------------------------------------------------------------------------------------------
worm v/s virus v/s trojan horse

https://www.websecurity.digicert.com/security-topics/difference-between-virus-worm-and-trojan-horse#:~:text=A%20Trojan%20horse%20is%20not%20a%20virus.&text=Unlike%20viruses%2C%20Trojan%20horses%20do,personal%20information%20to%20be%20theft.


https://www.geeksforgeeks.org/difference-between-virus-worm-and-trojan-horse/


-------------------------------------------------------------------------------------------------------------------------------------------

What is a Kernel? 

A Kernel is a computer program that is the heart and core of an Operating System. Since the Operating System has control over the system so, the Kernel also has control over everything in the system. It is the most important part of an Operating System. Whenever a system starts, the Kernel is the first program that is loaded after the bootloader because the Kernel has to handle the rest of the thing of the system for the Operating System. The Kernel remains in the memory until the Operating System is shut-down.

The Kernel is responsible for low-level tasks such as disk management, memory management, task management, etc. It provides an interface between the user and the hardware components of the system. When a process makes a request to the Kernel, then it is called System Call.

A Kernel is provided with a protected Kernel Space which is a separate area of memory and this area is not accessible by other application programs. So, the code of the Kernel is loaded into this protected Kernel Space. Apart from this, the memory used by other applications is called the User Space. As these are two different spaces in the memory, so communication between them is a bit slower.


System Library − System libraries are special functions or programs using which application programs or system utilities accesses Kernel's features. These libraries implement most of the functionalities of the operating system and do not require kernel module's code access rights.


Kernel Mode vs User Mode: 

Kernel component code executes in a special privileged mode called kernel mode with full access to all resources of the computer. This code represents a single process, executes in single address space and do not require any context switch and hence is very efficient and fast. Kernel runs each processes and provides system services to processes, provides protected access to hardware to processes.

Support code which is not required to run in kernel mode is in System Library. User programs and other system programs works in User Mode which has no access to system hardware and kernel code. User programs/ utilities use System libraries to access Kernel functions to get system's low level tasks.

-------------------------------------------------------------------------------------------------------------------------------------------

Difference between Microkernel v/s Monolithic Kernel: 

The main differences were listed as the following:

# The basic point on which microkernel and monolithic kernel is distinguished is that microkernel implement user services and kernel services in different address spaces and monolithic kernel implement both user services and kernel services under same address space.

# The size of microkernel is small as only kernel services reside in the kernel address space. However, the size of monolithic kernel is comparatively larger than microkernel because both kernel services and user services reside in the same address space.

# The execution of monolithic kernel is faster as the communication between application and hardware is established using the system call. On the other hands, the execution of microkernel is slow as the communication between application and hardware of the system is established through message passing.

# It is easy to extend microkernel because new service is to be added in user address space that is isolated from kernel space, so the kernel does not require to be modified. Opposite is the case with monolithic kernel if a new service is to be added in monolithic kernel then entire kernel needs to be modified.

# Microkernel is more secure than monolithic kernel as if a service fails in microkernel the operating system remain unaffected. On the other hands, if a service fails in monolithic kernel entire system fails.

# Monolithic kernel designing requires less code, which further leads to fewer bugs. On the other hands, microkernel designing needs more code which further leads to more bugs.

This can result in a gross generalisation.

# Microkernel slower and more secure.
# Monolithic kernel faster and less secure.

-------------------------------------------------------------------------------------------------------------------------------------------

What is thrashing?
Thrashing is a phenomenon in virtual memory scheme when the processor spends most of its time in swapping pages, rather than executing instructions.

-------------------------------------------------------------------------------------------------------------------------------------------

27) What is fragmentation?

As processes are loaded and removed from memory, the free memory space is broken into little pieces. It happens after sometimes that processes cannot be allocated to memory blocks considering their small size and memory blocks remains unused. This problem is known as Fragmentation.

Fragmentation is of two types −
	
1. External fragmentation: Total memory space is enough to satisfy a request or to reside a process in it, but it is not contiguous, so it cannot be used.

2. Internal fragmentation: Memory block assigned to process is bigger. Some portion of memory is left unused, as it cannot be used by another process.

Note: Can happen in both secondary and main memory.

-------------------------------------------------------------------------------------------------------------------------------------------
Types of OS :


Batch operating system : 

Some computer processes are very lengthy and time-consuming. To speed the same process, a job with a similar type of needs are batched together and run as a group. The user of a batch operating system never directly interacts with the computer. In this type of OS, every user prepares his or her job on an offline device like a punch card and submit it to the computer operator.

The problems with Batch Systems are as follows −

Lack of interaction between the user and the job.
CPU is often idle, because the speed of the mechanical I/O devices is slower than the CPU.
Difficult to provide the desired priority.



Time-sharing operating systems: 

Time-sharing is a technique which enables many people, located at various terminals, to use a particular computer system at the same time. Time-sharing or multitasking is a logical extension of multiprogramming. Processor's time which is shared among multiple users simultaneously is termed as time-sharing.

Multiple jobs are executed by the CPU by switching between them, but the switches occur so frequently. Thus, the user can receive an immediate response. For example, in a transaction processing, the processor executes each user program in a short burst or quantum of computation. That is, if n users are present, then each user can get a time quantum. When the user submits the command, the response time is in few seconds at most.

Advantages of Timesharing operating systems are as follows −

# Provides the advantage of quick response.
# Avoids duplication of software.
# Reduces CPU idle time.

Disadvantages of Time-sharing operating systems are as follows −

# Problem of reliability.
# Question of security and integrity of user programs and data.
# Problem of data communication.



Distributed operating System:

Distributed systems use multiple central processors to serve multiple real-time applications and multiple users. Data processing jobs are distributed among the processors accordingly.

The processors communicate with one another through various communication lines (such as high-speed buses or telephone lines). These are referred as loosely coupled systems or distributed systems. Processors in a distributed system may vary in size and function. These processors are referred as sites, nodes, computers, and so on.

The advantages of distributed systems are as follows −

# With resource sharing facility, a user at one site may be able to use the resources available at another.
# Speedup the exchange of data with one another via electronic mail.
# If one site fails in a distributed system, the remaining sites can potentially continue operating.
# Better service to the customers.
# Reduction of the load on the host computer.
# Reduction of delays in data processing.

Disadvantages:

# Security problem due to sharing
# Bandwidth is another problem if there is large data then all network wires to be replaced which tends to become expensive.
# If there is a database connected on local system and many users accessing that database through remote or distributed way then performance 	become slow



Network operating System:

A Network Operating System runs on a server and provides the server the capability to manage data, users, groups, security, applications, and other networking functions. The primary purpose of the network operating system is to allow shared file and printer access among multiple computers in a network, typically a local area network (LAN), a private network or to other networks.

The advantages of network operating systems are as follows −

# Centralized servers are highly stable.
# Security is server managed.
# Upgrades to new technologies and hardware can be easily integrated into the system.
# Remote access to servers is possible from different locations and types of systems.

The disadvantages of network operating systems are as follows −

# High cost of buying and running a server.
# Dependency on a central location for most operations.
# Regular maintenance and updates are required.



Real Time operating System:

A real-time system is defined as a data processing system in which the time interval required to process and respond to inputs is so small that it controls the environment. The time taken by the system to respond to an input and display of required updated information is termed as the response time. So in this method, the response time is very less as compared to online processing.

Real-time systems are used when there are rigid time requirements on the operation of a processor or the flow of data and real-time systems can be used as a control device in a dedicated application. A real-time operating system must have well-defined, fixed time constraints, otherwise the system will fail. For example, Scientific experiments, medical imaging systems, industrial control systems, weapon systems, robots, air traffic control systems, etc.

There are two types of real-time operating systems.

Hard real-time systems:

Hard real-time systems guarantee that critical tasks complete on time. In hard real-time systems, secondary storage is limited or missing and the data is stored in ROM. In these systems, virtual memory is almost never found.

Soft real-time systems:

Soft real-time systems are less restrictive. A critical real-time task gets priority over other tasks and retains the priority until it completes. Soft real-time systems have limited utility than hard real-time systems. For example, multimedia, virtual reality, Advanced Scientific Projects like undersea exploration and planetary rovers, etc.


-------------------------------------------------------------------------------------------------------------------------------------------

What is spooling?

Spooling : Simultaneous peripheral operation online, acronym for this is Spooling. A spool is a kind of buffer that holds the jobs for a device till the device is ready to accept the job. Spooling considers disk as a huge buffer that can store as many jobs for the device till the output devices are ready to accept them.

Buffering : The buffer is an area in the main memory that is used to store or hold the data temporarily that is being transmitted either between two devices or between a device or an application. In simple words, buffer temporarily stores data that is being transmitted from one place to another. The act of storing data temporarily in the buffer is called buffering.Buffering helps in matching the speed between the sender and receiver of the data stream. 

Spooling vs Buffering

# The key difference between spooling and buffering is that Spooling can handle the I/O of one job along with the computation of an another   
  job at the same time while buffering handles I/O of one job along with its computation.

# The buffer is a limited area in main memory while Spool uses the disk as a huge buffer.


-------------------------------------------------------------------------------------------------------------------------------------------

Registers:

Registers are a type of computer memory used to quickly accept, store, and transfer data and instructions that are being used immediately by the CPU. The registers used by the CPU are often termed as Processor registers. A processor register may hold an instruction, a storage address, or any data (such as bit sequence or individual characters). The computer needs processor registers for manipulating data and a register for holding a memory address. The register holding the memory location is used to calculate the address of the next instruction after the execution of the current instruction is completed.

-------------------------------------------------------------------------------------------------------------------------------------------

Program counter:

The “program counter,” also called the “instruction pointer,” keeps track of where a computer is in its program sequence. 

-------------------------------------------------------------------------------------------------------------------------------------------

Stack v/s Heap : 

Stack Allocation : The allocation happens on contiguous blocks of memory. We call it stack memory allocation because the allocation happens in function call stack. The size of memory to be allocated is known to compiler and whenever a function is called, its variables get memory allocated on the stack. And whenever the function call is over, the memory for the variables is deallocated. This all happens using some predefined routines in compiler. Programmer does not have to worry about memory allocation and deallocation of stack variables.

Heap Allocation : The memory is allocated during execution of instructions written by programmers. Note that the name heap has nothing to do with heap data structure. It is called heap because it is a pile of memory space available to programmers to allocated and de-allocate. If a programmer does not handle this memory well, memory leak can happen in the program.

Key Differences Between Stack and Heap Allocations

In a stack, the allocation and deallocation is automatically done by whereas, in heap, it needs to be done by the programmer manually.

# Handling of Heap frame is costlier than handling of stack frame.
# Memory shortage problem is more likely to happen in stack whereas the main issue in heap memory is fragmentation.
# Stack frame access is easier than the heap frame as the stack have small region of memory and is cache friendly, but in case of heap frames  	 which are dispersed throughout the memory so it cause more cache misses.
# Stack is not flexible, the memory size allotted cannot be changed whereas a heap is flexible, and the allotted memory can be altered.
# Accessing time of heap takes is more than a stack.

-------------------------------------------------------------------------------------------------------------------------------------------

Threads are sometimes called lightweight processes because they have their own stack but can access shared data. Because threads share the same address space as the process and other threads within the process, the operational cost of communication between the threads is low, which is an advantage. The disadvantage is that a problem with one thread in a process will certainly affect other threads and the viability of the process itself.

PROCESS             					THREAD

Processes are heavyweight operations	                Threads are lighter weight operations

Each process has its own memory space	                Threads use the memory of the process 
                                                        they belong to

Inter-process communication is slow as                  Inter-thread communication can be faster 
processes have different memory addresses               than inter-process communication because  
                                        	        threads of the same process share memory 
                                                        with the process they belong to


Context switching between processes is              	Context switching between threads of the 
more expensive 			 	                same process is less expensive  

Processes don’t share memory with other        		Threads share memory with other threads 
processes                                               of the same process

-------------------------------------------------------------------------------------------------------------------------------------------

Virtual Memory:


A computer can address more memory than the amount physically installed on the system. This extra memory is actually called virtual memory and it is a section of a hard disk that's set up to emulate the computer's RAM.

The main visible advantage of this scheme is that programs can be larger than physical memory. Virtual memory serves two purposes. First, it allows us to extend the use of physical memory by using disk. Second, it allows us to have memory protection, because each virtual address is translated to a physical address.

Advantages:

# Certain options and features of a program may be used rarely.

# The ability to execute a program that is only partially in memory would counter many benefits.

# Less number of I/O would be needed to load or swap each user program into memory.

# A program would no longer be constrained by the amount of physical memory that is available.

# Each user program could take less physical memory, more programs could be run the same time, with a corresponding increase in CPU  utilization and throughput.


Virtual memory is commonly implemented by demand paging. It can also be implemented in a segmentation system. Demand segmentation can also be used to provide virtual memory.


Demand Paging:

A demand paging system is quite similar to a paging system with swapping where processes reside in secondary memory and pages are loaded only on demand, not in advance. When a context switch occurs, the operating system does not copy any of the old program’s pages out to the disk or any of the new program’s pages into the main memory Instead, it just begins executing the new program after loading the first page and fetches that program’s pages as they are referenced.

While executing a program, if the program references a page which is not available in the main memory because it was swapped out a little ago, the processor treats this invalid memory reference as a page fault and transfers control from the program to the operating system to demand the page back into the memory.

-------------------------------------------------------------------------------------------------------------------------------------------

Relocation register v/s Limit register

The address generated by the CPU is a logical address which is not known to the Main Memory. The Physical memory or the Main Memory knows only the Physical address.

Therefore the address generated by the CPU(Logical Address) is compared with a Limit Register and if the value is low, It is added to the value in Relocation Register to yield a Physical Address.

In simple,

1) CPU generates Logical address(say 345)

2) 345 is compared with value in Limit Register.

3) If 345 > Limit Register then TRAP

4) If 345 < Limit Register then value is added with Relocation Register(say 300) to get physical address. ie: 345+300 = 645(Physical address)

Therefore if CPU can generate logical address in the range 0 to 345, then the corresponding physical address can range from R+0 to R+345 where R is the value in Relocation Register.

So, the address a user sees and the address a RAM sees to access a data are different.

-------------------------------------------------------------------------------------------------------------------------------------------


Single partition allocation v/s Multiple partion allocation

Single partition allocation: The entire program/process is loaded inside the main memory in a contiguous address space.

Multiple partion allocation: Main memory is divided into a number of fixed-sized partitions where each partition should contain only one process. When a partition is free, a process is selected from the input queue and is loaded into the free partition. When the process terminates, the partition becomes available for another process.


-------------------------------------------------------------------------------------------------------------------------------------------

 (TLB)

Memory Management technique:

Memory management is the functionality of an operating system which handles or manages primary memory and moves processes back and forth between main memory and disk during execution. Memory management keeps track of each and every memory location, regardless of either it is allocated to some process or it is free. It checks how much memory is to be allocated to processes. It decides which process will get memory at what time. It tracks whenever some memory gets freed or unallocated and correspondingly it updates the status.

Paging:

Paging is a memory management technique in which the memory is divided into fixed size pages. Paging is used for faster access to data. When a program needs a page, it is available in the main memory as the OS copies a certain number of pages from your storage device to main memory. Paging allows the physical address space of a process to be noncontiguous. Paging reduces external fragmentation, but still suffer from internal fragmentation.

A computer can address more memory than the amount physically installed on the system. This extra memory is actually called virtual memory and it is a section of a hard that's set up to emulate the computer's RAM. Paging technique plays an important role in implementing virtual memory.

Paging is a memory management technique in which process address space is broken into blocks of the same size called pages (size is power of 2, between 512 bytes and 8192 bytes). The size of the process is measured in the number of pages. Similarly, main memory is divided into small fixed-sized blocks of (physical) memory called frames and the size of a frame is kept the same as that of a page to have optimum utilization of the main memory and to avoid external fragmentation.

Address Translation: Page address is called logical address and represented by page number and the offset.

Logical Address = Page number + page offset

Frame address is called physical address and represented by a frame number and the offset.

Physical Address = Frame number + page offset

A data structure called page map table is used to keep track of the relation between a page of a process to a frame in physical memory.

When the system allocates a frame to any page, it translates this logical address into a physical address and create entry into the page table to be used throughout execution of the program.

When a process is to be executed, its corresponding pages are loaded into any available memory frames. Suppose you have a program of 8Kb but your memory can accommodate only 5Kb at a given point in time, then the paging concept will come into picture. When a computer runs out of RAM, the operating system (OS) will move idle or unwanted pages of memory to secondary memory to free up RAM for other processes and brings them back when needed by the program.

This process continues during the whole execution of the program where the OS keeps removing idle pages from the main memory and write them onto the secondary memory and bring them back when required by the program.

Segmentation:

Segmentation is a memory management technique in which each job is divided into several segments of different sizes, one for each module that contains pieces that perform related functions. Each segment is actually a different logical address space of the program.

When a process is to be executed, its corresponding segmentation are loaded into non-contiguous memory though every segment is loaded into a contiguous block of available memory.

Segmentation memory management works very similar to paging but here segments are of variable-length where as in paging pages are of fixed size.

A program segment contains the program's main function, utility functions, data structures, and so on. The operating system maintains a segment map table for every process and a list of free memory blocks along with segment numbers, their size and corresponding memory locations in main memory. For each segment, the table stores the starting address of the segment and the length of the segment. A reference to a memory location includes a value that identifies a segment and an offset.

-------------------------------------------------------------------------------------------------------------------------------------------

Translation Lookaside Buffer (TLB)

For each process page table will be created, which will contain Page Table Entry (PTE). This PTE will contain information like frame number.
This page table entry (PTE) will tell where in the main memory the actual page is residing.

Where to place the page table, such that overall access time (or reference time) will be less?

The idea used here is, place the page table entries in registers, for each request generated from CPU (virtual address), it will be matched to the appropriate page number of the page table, which will now tell where in the main memory that corresponding page resides. Everything seems right here, but the problem is register size is very small.

To overcome this problem a high-speed cache is set up for page table entries called a Translation Lookaside Buffer (TLB). Translation Lookaside Buffer (TLB) is nothing but a special cache used to keep track of recently used transactions. TLB contains page table entries that have been most recently used. Given a virtual address, the processor examines the TLB if a page table entry is present (TLB hit), the frame number is retrieved and the real address is formed. If a page table entry is not found in the TLB (TLB miss), the page number is used to index the process page table. TLB first checks if the page is already in main memory, if not in main memory a page fault is issued then the TLB is updated to include the new page entry.

-------------------------------------------------------------------------------------------------------------------------------------------

What are micro-processors?

It is a programmable device that takes in input perform some arithmetic and logical operations over it and produce desired output. In simple words, a Microprocessor is a digital device on a chip which can fetch instruction from memory, decode and execute them and give results.

Microprocessor performs three basic things while executing the instruction:

# It performs some basic operations like addition, subtraction, multiplication, division and some logical operations using its Arithmetic and Logical Unit (ALU). New Microprocessors also perform operations on floating point numbers also.
# Data in Microprocessor can move from one location to another.
# It has a Program Counter (PC) register that stores the address of next instruction based on the value of PC, Microprocessor jumps from one   location to another and takes decision.

Clock Speed of different Microprocessor:

16-bit Microprocessor –
8086: 4.7MHz, 8MHz, 10MHz
8088: more than 5MHz
80186/80188: 6MHz
80286: 8MHz 

32-bit Microprocessor –
INTEL 80386: 16MHz to 33MHz
INTEL 80486: 16MHz to 100MHz
PENTIUM: 66MHz 

64-bit Microprocessor –
INTEL CORE-2: 1.2GHz to 3GHz
INTEL i7: 66GHz to 3.33GHz
INTEL i5: 2.4GHz to 3.6GHz
INTEL i3: 2.93GHz to 3.33GHz 

-------------------------------------------------------------------------------------------------------------------------------------------

RAM?

RAM(Random Access Memory) is a part of computer’s Main Memory which is directly accessible by CPU. RAM is used to Read and Write data into it which is accessed by CPU randomly. RAM is volatile in nature, it means if the power goes off, the stored information is lost. RAM is used to store the data that is currently processed by the CPU. Most of the programs and data that are modifiable are stored in RAM.

Integrated RAM chips are available in two form:

SRAM(Static RAM) : Static memories(SRAM) are memories that consist of circuits capable of retaining their state as long as power is on.
		   costlier & fast

DRAM(Dynamic RAM): DRAM stores the binary information in the form of electric charges that applied to capacitors. 
		   cheaper & slow


-------------------------------------------------------------------------------------------------------------------------------------------

An object file is the real output from the compilation phase. It's mostly machine code, but has info that allows a linker to see what symbols are in it as well as symbols it requires in order to work. (For reference, "symbols" are basically names of global objects, functions, etc.)
A linker takes all these object files and combines them to form one executable (assuming that it can, ie: that there aren't any duplicate or undefined symbols). A lot of compilers will do this for you (read: they run the linker on their own) if you don't tell them to "just compile" using command-line options. (-c is a common "just compile; don't link" option.)

Loading v/s Linking:

The key difference between linking and loading is that the linking generates the executable file of a program whereas, the loading loads the executable file obtained from the linking into main memory for execution. The linking combines all object files of a program to generate executable modules it also links the library function in the object module to built-in libraries of the high-level programming language. On the other hand, loading allocates space to an executable module in main memory.

-------------------------------------------------------------------------------------------------------------------------------------------
		   
Static Linking:

When we click the .exe (executable) file of the program and it starts running, all the necessary contents of the binary file have been loaded into the process’s virtual address space. However, most programs also need to run functions from the system libraries, and these library functions also need to be loaded. In the simplest case, the necessary library functions are embedded directly in the program’s executable binary file. Such a program is statically linked to its libraries, and statically linked executable codes can commence running as soon as they are loaded.

Disadvantage:
Every program generated must contain copies of exactly the same common system library functions. In terms of both physical memory and disk-space usage, it is much more efficient to load the system libraries into memory only once. Dynamic linking allows this single loading to happen.

Dynamic Linking:
Every dynamically linked program contains a small, statically linked function that is called when the program starts. This static function only maps the link library into memory and runs the code that the function contains. The link library determines what are all the dynamic libraries which the program requires along with the names of the variables and functions needed from those libraries by reading the information contained in sections of the library. After which it maps the libraries into the middle of virtual memory and resolves the references to the symbols contained in those libraries. We don’t know where in the memory these shared libraries are actually mapped: They are compiled into position-independent code (PIC), that can run at any address in memory.

Advantage:
Memory requirements of the program are reduced. A DLL is loaded into memory only once, whereas more than one application may use a single DLL at the moment, thus saving memory space. Application support and maintenance costs are also lowered.

-------------------------------------------------------------------------------------------------------------------------------------------

what are spin-locks?

In a loop a thread waits simply ('spins') checks repeatedly until the lock becomes available. This type of lock is a spin lock. The lock is a kind of busy waiting, as the threads remains active by not performing a useful task. They are efficient as they are blocked only for a short periods. They can be wasteful if they are held for a longer duration.

-------------------------------------------------------------------------------------------------------------------------------------------

Cache Memory is a special very high-speed memory. It is used to speed up and synchronizing with high-speed CPU. Cache memory is costlier than main memory or disk memory but economical than CPU registers. Cache memory is an extremely fast memory type that acts as a buffer between RAM and the CPU. It holds frequently requested data and instructions so that they are immediately available to the CPU when needed.

Cache memory is used to reduce the average time to access data from the Main memory. The cache is a smaller and faster memory which stores copies of the data from frequently used main memory locations. There are various different independent caches in a CPU, which store instructions and data.

Levels of memory:

Level 1 or Register –
It is a type of memory in which data is stored and accepted that are immediately stored in CPU. Most commonly used register is accumulator, Program counter, address register etc.

Level 2 or Cache memory –
It is the fastest memory which has faster access time where data is temporarily stored for faster access.

Level 3 or Main Memory –
It is memory on which computer works currently. It is small in size and once power is off data no longer stays in this memory.

Level 4 or Secondary Memory –
It is external memory which is not as fast as main memory but data stays permanently in this memory.

-------------------------------------------------------------------------------------------------------------------------------------------

