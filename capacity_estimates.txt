
Thousand - KB - 10^3
Million  - MB - 10^6
Billion  - GB - 10^9
Trillion - TB - 10^12

5 yrs -> 150 Million seconds

Modern day server capacity -> 256 GB

Consistency can take a hit (in the interest of availability)

------------------------------------------------------------------
TinyURL:
------------------------------------------------------------------

500M new URL shortenings per month

100:1 read/write ratio

expect 50B redirections (100 * 500M => 50B)

Queries Per Second (QPS) : 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s (write)
							    100 * 200 URLs/s = 20K/s	   (read)

500 million * 5 years * 12 months = 30 billion

for 5 years: 500 million * 5 years * 12 months = 30 billion objects

storage for 5 yrs = 30 billion * 500 bytes(object size) = 15 TB


Bandwidth estimates: 200 * 500 bytes = 100 KB/s  ingress
					10 MB/s  egress


Memory estimates: 20K * 3600 seconds * 24 hours = ~1.7 billion

	
cache 20% of these requests: 0.2 * 1.7 billion * 500 bytes = ~170GB


# Key Generation Service + caching

-----------------------------------------------------------------
Pastebin: 
-----------------------------------------------------------------

5:1

10^6 W / day   -> 120 KB/s 
5*10^6 R / day -> 0.6 MB/s 

10^6 * 10 * 360 = 3.6 * 10^9 = 3.6B (pastes per 10 years)

Avg paste size 10KB

3.6B * 10*10^3 = 36 TB  (storage in 10 years)

3.6B * 6 = 22GB (6 bytes per keys) (key storage)

36 TB >> 22 GB  =  36 TB

0.7 -> 36 TB = 1 -> 51.4TB

KGS + key-Value(content-key,content) + s3 storage + caching

-----------------------------------------------------------------
Instagram: (Photo + likes + follow)
-----------------------------------------------------------------

500M total users
1M daily active users
2M new photos every day, 23 new photos every second.
Average photo file size => 200KB

Total space required for 1 day of photos: 2M * 200KB => 400 GB
Total space required for 10 years: 	  400GB * 365 (days a year) * 10 (years) ~= 1425TB

User:

     UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes 	  500 million * 68 ~= 32GB		

Photo:
     	PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) +
	PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate
	(4 bytes) = 284 bytes

	2M * 284 bytes ~= 0.5GB per day
	For 10 years = 1.88TB


UserFollow:
	500 million users * 500 followers * 8 bytes ~= 1.82TB

Total space required for all tables for 10 years: 32GB + 1.88TB + 1.82TB ~= 3.7TB


UserFollow/UserPhoto table -> Cassandra -> columnar storage of UserFollowID / UserPhotoID

Partition based on PhotoID (epoch time  + auto ++ keys)

-----------------------------------------------------------------
Dropbox:
-----------------------------------------------------------------

Let’s assume that we have 500M total users, and 100M daily active users (DAU).

Let’s assume that on average each user connects from three different devices.

On average if a user has 200 files/photos, we will have 100 billion total files.

Let’s assume that average file size is 100KB, this would give us ten petabytes of total storage.
100B * 100KB => 10PB



client , Block server ( ---> cloud storage) , Metadata server (------> metadata DB), synchronization server(------> metadata DB)

client:

I. Internal Metadata Database
II. Chunker 
III. Watcher
IV. Indexer


Synchronization Service :

Metadata DB(contains info about the modified chunks(their IDs & Hash values)), Message Queuing Service 




Cloud/Block Storage


Metadata Partitioning (hash based + consistent hashing)

Inline data deduplication

-----------------------------------------------------------------
Facebook Messenger:
-----------------------------------------------------------------

500 million daily active users

average each user sends 40 messages

20 billion messages per day

20 billion messages * 100 bytes => 2 TB/day

2 TB * 365 days * 5 years ~= 3.6 PB

2 TB / 86400 sec ~= 25 MB/s


-----------------------------------------------------------------
Twitter:
-----------------------------------------------------------------

200 million daily active users (DAU)


Ingress:

100 million new tweets every day

average size of text tweet = 300 Bytes (280 Bytes of character + 30 Bytes of metadata = 310 == 300)
average size of a tweet with photo = 200 KB
average size of a tweet with video = 2 MB

Frequency of text tweet = 100 M/day
Frequency of photo tweet = (100 M/day) / 5
Frequency of video tweet = (100 M/day) / 10


Total ingress = 100M * 300 bytes + (100M/5 photos * 200KB) + (100M/10 videos * 2MB) = 24TB/day == 290MB/sec

Egress:

Lets assume avrage user goes through 20 tweets every session with 7 sessions/ day,

Total tweets genearted = 200M DAU * ((2 + 5) * 20 tweets) => 28B/day

assume that the users watch every 3rd video they see in their timeline,

  (28B * 280 bytes) / 86400s of text => 93MB/s
+ (28B/5 * 200KB ) / 86400s of photos => 13GB/S
+ (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s
Total ~= 35GB/s

-----------------------------------------------------------------
YouTube:
-----------------------------------------------------------------

1.5 billion total users
800M daily active users
800M * 5 / 86400 sec => 46K videos/sec (user views 5 videos per day)

upload:view = 1:200
46K / 200 => 230 videos/sec

Storage Estimates:

500 hours getting uploaded every hours: 500 hours * 60 min * 50MB => 1500 GB/min (25 GB/sec)

Bandwidth Estimates:

assuming each video upload takes a bandwidth of 10MB/min
500 hours * 60 mins * 10MB => 300GB/min (5GB/sec)

read:write ratio to be 200:1

-----------------------------------------------------------------
Typeahead
-----------------------------------------------------------------

5 billion searches/ day

assusming we have 100 million chars to build an index

Lets assume 15 chars to be an average typehead suggestion, 

storage estimation: 100M*2B*15 => 3GB

assuming 2% new queries everyday,

storage estimation: 0.02*365*3GB => 25 GB/ year

-----------------------------------------------------------------
API Rate Limiter  (N/A)
-----------------------------------------------------------------
-----------------------------------------------------------------
Twitter
-----------------------------------------------------------------

Total users : 1.5 B
DAU : 800 M
avg size of a tweet : 300 bytes
number of searches/day : 500 M
number of new tweets/day : 400 M

Storage Capacity: 400M * 300 => 120GB/day ~= 1.38MB/second

120GB * 365days * 5years ~= 200TB  (storage for 5 yrs)


-----------------------------------------------------------------
Web Crawler
-----------------------------------------------------------------

crawl 15 billion pages within four weeks

15B / (4 weeks * 7 days * 86400 sec) ~= 6200 pages/sec

15B * (100KB + 500) ~= 1.5 petabytes  (100 kb size of avg HTML text)

1.5 petabytes / 0.7 ~= 2.14 petabytes  (actual storage assuming  a 70% capacity model)

-----------------------------------------------------------------
Facebook Newsfeed
-----------------------------------------------------------------

Traffic Estimates:

DAU: 300 M

avg timeline fetch/ day : 5

Total fetched timelines/ day : 1.5 B ~= 17500/sec


Storage estimates:

500 posts / day seen by the user

500 KB of data seen / day / user

500 KB * 300 M = 150 TB

1500 servers needed (modern day server can hold rougly 100 GB of data)

-----------------------------------------------------------------
Yelp
-----------------------------------------------------------------

places: 500M
queries/sec: 100K

-----------------------------------------------------------------
Uber Backend
-----------------------------------------------------------------

DAU Drivers : 500K
DAU Customers : 1M

DriverLocationHT memory:

1 million * 35 bytes(DriverID, Old latitude, Old longitude, New latitude, New longitude) => 35 MB
-----------------------------------------------------------------
Ticketmaster
-----------------------------------------------------------------

page views : 3B
tickets sold : 10 M

50 bytes (IDs, NumberOfSeats, ShowID, MovieID, SeatNumbers, SeatStatus, Timestamp, etc.) to store in the database.
We would also need to store information about movies and cinemas; let’s assume it’ll take 50 bytes.

500 cities * 10 cinemas * 2000 seats * 2 shows * (50+50) bytes = 2GB / day

To store five years of this data, we would need around 3.6TB.









 













 























