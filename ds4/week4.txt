# video:1

The max cut problem is an NP complete problem. Bipartite graph is a special case where it can be solved using BFS. A local seacrh algorithm for max-cut problem is as
follows:

Let (A,B) be an arbitrary cut of a graph.

Cv(A,B)= no. of edges incident on vertex 'v' which cross the cut.
Dv(A,B)= no. of edges incident on vertex 'v' which dont cross the cut.

While there is a vertex V with Dv >= Cv , move V to the other side of the cut.

return the final cut(A,B).

This algorithm terminates after nC2 iterations(as the total no. of edges are nC2).

Performance guarantee:

This local search algorithm outputs a cut in which the no. of crossing edges are atleast 50% of maximum possible. Infact it outputs a cut having crossing edges more than
50% of total no.of edges of the graph.

#video:2

        sigma(for all Vs)Dv >= sigma(for all Vs)Cv
=>   2(no. of non crossing edges)>= 2(no. of crossing edges)                       (every edge will get counted twice in  both the cases)
=>   2(|E|)>=4(no. of crossing edges)
=>   no. of crossing edges <= (1/2)(|E|)

The weighted version of the same problem can also be solved in the same way. Here Cv would be the sum of the weights of the edges crossing the cut and incident on V,
Dv would be the sum of the weights of the edges not crossing the cut and incident on V. We proceed just like above.

The algorithm doesnt complete in nC2 iterations though.

# video:3

Neighbourhoods in a local search algorithm:

Let X be the set of a candidate solutions. e.g cuts of a graph , TSP tour , constraints satifiability assignments.
for each x E X , specify which y E X are its neighbours.

e.g  In maximum cut problem , neighbour to a cut was the cut generated by moving one vertex to the other side.
     In constraints satifiability problems its the assignment generated by flipping one of the variable to its negation.
     In TSP tours neighbour is the tour generated by difference in 2 edges.(see snapshot of the video at 02:20) 

A generic local search algorithm is as follows:

Have a random solution X

If there is a neighboring solution Y beter than X then set X=Y

continue till there is no neighbour better than X

return X

How to pick an initial solution?

i> Simply pick a random solution and have many trials of local search on it and return the optimal most solutiuon.
ii> If u have a good heuristic available then local search can be applied as a post processing step to the final solution produced by heuristic.

What if many superior neighborhoods are present?

In this we may pick a neighbour at random , other than the first step of a local search algorithm where we ran the local search many a times on random different
initial solutions , this step of picking up a random neighbour injects randomness back again.

Other than that we may pick the best neighbour among all.

It depends on the domain that you are working upon to decide which of the above two methodologies to adopt.

How to define the neighbourhoods?

e.g what if we swapped two pair of vertices in each iteration of the max cut problem , this may lead to a larger running time but accuracy increases with such measures
in general.

Basically we have to find a sweet spot considering the optimality of the solution and the running time which we can bear.

Is local search guaranteed to terminate?

If X is finite then yes.

Is local search guaranteed to converge quickly?

Usually not but does well in practice.

Are locally optimal solutions generally good approximations to globally optimal answers?

No, thats why we have to run this algorithm many a times with random initial solutions.

#video:5

Ways to solve a 2-SAT problem having n variables in polynomial time:

i>   Reduction to computiong strongly connected components.
ii>  Backtracking , Here we pick a value for a random variable and look out for possible assignments.
iii> Local search (Discussed below)

3-SAT is an NP-Complete problem. We can get away with O((4/3)^n) instead of brute force which takes O(2^n).

#video:6

Random walks on a straight line:

Let there be a path named as follows,

0...1...2...3...4...5.......................(n-1)...(n)...(n+1)

We have to go to position 'n' from position 0 provided the constraint that there is a 50-50 chance that we go left or right. If the position is 0 then there is a
100% chance that we are gonna go right.

Let Tn denote the total no. walks to reach 'n'.

Let Zi denote the no. of walks from ith position to 'n'. (Z0=Tn)

If we are already at n then we need 0 no. of walks , so

E[Zn]=0
Also if we are extreme left i.e at 0 then we will surely move right, So

E[Z0]=1+E[Z1]


Consider our position to be i.

E[Zi]= (1/2)*E(Zi|go left) + (1/2)*E(Zi|go right)
     = (1/2)(1 + E(Zi-1)) + (1/2)(1 + E(Zi+1))

or, E(Zi)-E(Zi+1) = E(Zi-1)-E(Zi) + 2

The difference of difference increases by two as i increases , this can be written as,

E[Z0]-E[Z1]=1  (we already know)
E[Z1]-E[Z2]=3
.
.
.
.
E[Zn-1]-E[Zn]=2n-1

Adding everything we get,

E[Z0]-E[Zn]=n^2

since E[Zn]=0

therefore, E[Z0]=n^2=Tn

Expected no. of walks to reach n is n^2

Corollary:

Pr[Tn > 2n^2]  <= 1/2

proof:

n^2 = E[Tn] = sigma(k=0 to 2n^2) K * Pr(Tn=k)   +  sigma(k=2n^2 to infinity)  K * Pr(Tn=k)
            =                           (>=0)                                         (k can be lower bounded by 2n^2)
            >=     2n^2 Pr[Tn>2n^2]

therefore , Pr[Tn > 2n^2] <= 1/2

#video:7

Let there be an arbitrary satisfying assingnment to a 2-SAT instance A* (key note: There can be more than one satisfying assignment)

Let At be our assignment after t iterations of inner loop. Let Xt denotes the no. of assignments of At which resembles A*. We are gonna halt when all the assignments
resembles with that of A* or in other words Xt=n.

Let pick a clause having variables Xi and Xj inside,

If Xi and Xj both have values opposite to their values in A* then flipping any one of them makes that clause work 100% of times(Xt+1 = 1 + Xt).

If one of them is having resemblance while the other one is not. Then flipping may lead us to trouble or may satisfy the clause . (Xt+1) could be (Xt - 1) or (Xt + 1) 
with half probability.

So random variables Xi's behave like random walks except for the case,

i>   Sometime moves forward with 100%(when Xi and Xj both have values opposite to their values in A* ) instead of 50%.
ii>  X0>=0 i.e its very unlikely that the asiignment we started off with is having all the values opposite to what it is in A*.
iii> Since there are many possibilities for A* , we may end up earlier i.e Xn < n.

Basically random variables Xi's behave like random walks in the worst case i.e we start off with assignments which are opposite to what it should be and Xn takes 
over all the values of n.

In our last corollary we stated that Pr[Tn > 2n^2] <= 1/2 , here by analogy we can say that we fail with the probability <= 1/2

i.e Pr[that we fail in a single iteration of outer loop] <= 1/2
    Pr[that we fail in log(base2)(n)iteration of outer loop]  <= (1/2)^log(base2)(n)
                                                              <=  1/n
    
#video:8

Stable matching: see video(didn't feel like writing)

#video:9

Stable matching in a bipartite graph reduces to maximum flow problem. See braces paradox in the later part of the video.

#video:

open source solvers or commercial LP solvers , convex programming (for solving LPP) 
